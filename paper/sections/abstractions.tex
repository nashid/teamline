\documentclass[../manifest.tex]{subfiles}

% - you should analyze your domain problem according to the framework of the book, translating from domain language into abstract descriptions of both tasks and data
% - typically data will need to come first, since you will need to refer to that data in your task descriptions.
% - it is very likely that you will need to first have domain-specific descriptions, followed by the abstracted versions.
% - it is often helpful to split these sections into two pieces: first have subsection where data is described in domain-specific terms, and then follow up with a next subsection where you've abstracted into domain-independent language. you may choose to have a separate Domain Background section before this one if there's a lot of specialized vocabulary/material to explain in order to have a comprehensible and concise data description.
% - similarly, it can be easier to write the tasks section by first providing a domain-specific list of tasks, and then present the abstracted version.
% - you should decide whether to split by domain vs abstract (first have both data and task domain-specific sections, followed by the abstractions for data and task) or to split by data/task (first have data domain-specific then data abstract, then have task domain-specific and finally task abstract).






\begin{document}
\subsection{Domain Background}
Our tool uses data collected by AutoTest\footnote{http://github.com/nickbradley/autotest},
an automatic grading service used to grade code submissions for students in
CPSC310. The course is structured around a term-long coding project that is
divided into 5 deliverables (or sprints) completed by teams consisting of 2-3 students.
The first 3 of these deliverables are graded by a combination of AutoTest and TAs.
Teams manage their shared code on GitHub\footnote{http://github.com}
using a basic git workflow: students pull the latest code changes from GitHub,
commit their modified code locally and then push those commits to GitHub for
other members to see. Every time a student pushes their changes, AutoTest is
automatically invoked and runs a suite of instructor-written tests against the modified
code. In addition, it computes the amount of code covered by student-written tests. Results are stored in a NoSQL database with each record corresponding to a
single submission (push event).


\subsection{Data Description} \label{ssec:data-description}
The grade dataset is treated as a static table which contains over 44,000 submission records for 285 students in 139 teams. From this dataset we use the attributes shown in table \ref{tab:attributes}. From those attributes we created the following derived attributes:

\textbf{Pass rate contribution.} A student's commit is considered as contributing to the team's pass rate if one or more instructor-written tests are passed for the first time in this commit. The contribution is calculated as the number of tests passing for the first time divided by the total number of tests passed by the team. We do not visualize this value value directly. Instead, we visualize the accumulated value over all commits to ensure that the plotted points are monotonic. This is a quantitative attribute ranging from 0-100\%.

\textbf{Coverage contribution.} We consider a contribution to the coverage grade to occur when a commit increases the coverage grade beyond anything seen so far. The amount of the  contribution is the amount by which which it exceeds the running maximum. Again, we accumulate this value to ensure a monotonic line. Note that the coverage is increased when a student writes (more) tests that execute a higher proportion of their code or they change the total lines of code. This is a quantitative attribute ranging from 0-100\%.

\textbf{Overall contribution.} The overall contribution is computed as the sum of 80\% pass rate contribution and 20\% coverage contribution to match the weights used in computing the deliverable grade. This attribute is quantitative and ranges between 0 and 100\%.

\textbf{Within-team contribution uniformity (CU).} This measures how evenly the team members contributed. It is a quantitative attribute that ranges from 0 to 1 where 0 indicates that one team member did all the work while 1 indicates a completely uniform workload split (each member contributed equally). This attribute is computed by taking the sorted pairwise difference of the overall contribution (computed as 80\% test grade contribution and 20\% coverage grade contribution) of each team member. More specifically, if $m$ is the number of team members and $u_i$ is the overall contribution for the $i$th team member (sorted by overall contribution), then we have

\begin{equation}
  \label{eq:con-uniform}
  CU = 1-\sum_{i=1}^{m-1} |u_i - u_{i+1}|.
\end{equation}


\begin{table*}[t]
  \centering
  \begin{threeparttable}
      \caption{Dataset Attributes.}
      \label{tab:attributes}
  \begin{tabular*}{\textwidth}{lll}
    \hline
    \textbf{Attribute Name} & \textbf{Attribute Type} & \textbf{Description} \\
    \hline
    pass rate     & Quantitative & Percentage of instructor-written tests that passed against student code. \\
    coverage  & Quantitative & Percentage of student code executed by student-written tests. \\
    grade     & Quantitative & Computed as 80\% pass rate + 20\% coverage. \\
    timestamp      & Sequential   & Unix time of record creation. \\
    commitID      & Categorical  & The (truncated) SHA-1 hash of the submitted commit. \\
    githubID      & Categorical\tnote{a}  & The GitHub ID of the student making the submission. \\
    team           & Categorical\tnote{b}  & The team number, stored as \textit{teamXXX}, where \textit{XXX} is a number between 2 and 199. \\
    deliverable    & Sequential   & The submission deliverable, which can have values \textit{d1}, \textit{d2}, or \textit{d3}. \\
    \hline
  \end{tabular*}
  \begin{tablenotes}\footnotesize
    \item [a] 285 values currently; max $<1000$.
    \item [b] 139 values currently; max $<1000$.
\end{tablenotes}
\end{threeparttable}
\end{table*}


\subsection{Task Description}
After a submission deadline, TAs meet with their assigned
teams to conduct a retrospective to discuss any challenges that arose
during the sprint and to ensure that the work was equitably distributed among the
team members. This typically consists of a TA asking some questions designed to
gauge a student's comprehension of the task and code. They may go so far as to
explicitly and privately ask each student how evenly they felt the workload was
split. Based on the retrospective, the TAs assign a scaling factor to the deliverable grade. For example, if
the team got 90\% on the deliverable but one member did most of the work, the final
grades might be 90\%*1.0 = 90\% and 90\%*0.6 = 54\%. Unfortunately, it can be hard
to determine how much work was done by each student from these conversations since
the team member who contributed very little will attempt to spoof the TA while the
hard-working one may not want to rat out their partner. One possible solution is
to look at the commit history on GitHub to determine how many commits each student
made. This can be a decent proxy but can be misleading since different people have
different commit habits (some will commit every line, others only large changes)
and they may not reflect the actual contribution to the grade (i.e. commits that
don't directly increase the grade).

From the description, we can identify two distinct tasks that a TA is expected to perform. The first is to understand the uniformity of the contributions. To support this task the visualization needs to present the contributions to the user in a way that makes it apparent if contributions of one person where much larger or much smaller than the others. The second task is to summarize the commit history of the team. We discuss how Teamline line enables these tasks in the next section.

\end{document}
